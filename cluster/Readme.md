﻿# 实验报告
本次实验是在sklearn库中自带的两个数据集上进行各种聚类方法的测试，并对聚类出的结果的各种指标进行分析。  
### 1.数据分析  
本次实验所使用的两个数据集分别是handwritten digits data和20newsgroups data，第一个手写数字数据集包含了1797个样本，有5个属性，分别是images，data，target_names，DESCR，target，其中images是ndarray类型，维度为8*8，data是将images展开为一行，所以一共有1797行，这是实验主要使用的数据。target属性表示每一个样本的标签，target_names表示所有标签的集合，也就是[0-9]这10个标签的集合，而DESCR则是关于数据集的描述。第二个数据集则是涉及到20中话题的一万多篇新闻文章，所以称作20newsgroups，和上述数据集不同的是，它是文本的数据集，所以在聚类前需要对数据进行处理，而且这个数据集可以选择使用训练集或测试集或一起使用以及选取想要测试的类别，选取完之后的属性与第一个数据集合相似。  
### 2.数据处理
对于上述两个数据集，在聚类时其实都需要进行相应的处理。对于第一个数据集，我们可以发现它的维度过大，高维的数据集在普通的聚类方法中普遍没有较好的效果。所以在这次实验中测试了对数据集进行降维进行聚类，在代码中注释掉了，结果是对没有进行降维处理数据的聚类结果。第二个数据集由于是文本信息，所以一般要先将文本转换为向量，实验中使用TfidfVectorizer将文本转换为TF-IDF向量，当然在官方文档中还使用了HashingVectorizer，在代码中也有使用但是注释了，当获得了TF-IDF向量后，并不适合直接使用聚类方法，因为提取的向量过于稀疏，所以学习官方文档使用了LSA完成了对稀疏向量的奇异值分解以及降维过程，这样既去除了一些噪音，也就是无关信息，又使得矩阵维数降低，有利于获得更好的聚类效果，之后归一化也是为了更好的聚类效果。
### 3.方法介绍
本次实验所使用的8种聚类方法，其实也可以说成7种，在其中的层次聚类hierarchical clustering中就包含了自底向上的Agglomerative clustering。  
首先是K-Means算法，这个方法的原理其实就是初始化几个聚类中心，然后不停地重新计算，然后在某一条件下停止而已，某一条件可能是迭代次数或者是误差平方和小于某个阈值。这个方法地使用也十分简单，只需要传入类别数目，然后fit数据集即可，虽然kmeans算法简单、收敛快调参容易，但是它得到的都是局部最优，而且对数据中的噪声异常值十分敏感，而且很容易看出，在实验所用的高维数据上效果不好。  
Affinity propagation，也就是常说的AP聚类算法，它与KMeans不同，它不需要提前给的类的数目，它把每一个数据点都作为潜在的聚类中心，它引入了吸引度矩阵R和吸引度矩阵A的不断迭代，来确定最终的聚类中心，这个方法不需要事先知道类的数目，而且对初始值不敏感，但是由于这个算法需要对两个矩阵进行更新，算法复杂度较高，所以在数据量大的情况下，耗费的时间会很长。  
Mean-shift，均值漂移，设定参数bandwidth，这个方法会初始化一个点center，然后找出距离center小于bandwidth的点作为一个集合，然后用center到集合中的点的向量的均值作为shift，令center移动，再把之后所含的点与之前的归为一个簇，一直迭代直到shift迭代到收敛，这些点归为一个簇，重复直到所有点被访问。这个方法还可以加入高斯权重，改变了shift的计算。它也不需要事先给定类的数目，不过它的时间耗费也很长，在这个实验中的效果也不是很好。  
Spectral clustering谱聚类，其实是一种特殊的聚类。它其实本质上是先对数据进行特征的提取，可以理解为一种降维，然后使用普通的聚类方法进行聚类。在它的实现过程中使用了特征值分解，其实与之前我先对数据进行主成分分析然后聚类的思想一样，它J将样本从原始空间变换，在投影空间中可以获得刚好的聚类效果。
Ward hierarchical clustering和Agglomerative clustering在这可以一起进行描述，Agglomerative clustering是层次聚类hierarchical clustering的一种，hierarchical clustering层次聚类包含两种，一种是自顶向下，一种是自底向上，而
Agglomerative clustering属于后一种，本次实验也是使用它。自底向上，顾名思义，一开始每个点都作为一个簇，然后根据相似度对点进行合并直到簇为给定数目为止，而Ward是一种linkage type，即是一种距离的定义，像single即不同簇的最近点邻近度，complete即不同簇最远点邻近度，group average即不同簇所有点邻近度的均值，centroid即质心距离，ward是先将两个簇合并后的质心求出，再求出两个簇的点到这个质心距离的平方和，其实是十分便于理解的一个方法。  
DBSCAN，是基于密度的聚类算法中较为经典的一种，上课也说过，它通过设定MinPts和EPS以及密度可达以及密度连通的定义，通过密度实现了聚类。这个方法对抗噪声的效果很好，它可以标出噪声点，同时可以处理不同形状的簇，这些都是有些聚类方法如kmeans难以实现的，而且不用设置类别数目，但是它需要设定EPS和MinPts，这两个参数的指定就十分麻烦了，因为这两个参数是相互依赖的，所以在设定上较难。  
Gaussian mixtures，核心是高斯混合模型，也就是常说的GMM。这个模型通常是用于几个分布数据混合的处理，在第一个数据集上就可以看作10中不同高斯分布的数据混合在一起，此时用于单高斯分布模型的最大似然估计就没有用处了，这里原理是使用EM算法，在报告里就不展开了，反正它通过E-step和M-step多次使得Q函数最大，所以也叫作最大化Q函数，最终得到最后的参数。这个方法与其他聚类方法最大的不同是，它不是简单地把一个数据分为一类，而实给出了一个数据是每个类的概率，所以这个方法不仅仅可以用来聚类，它的本质是一个生成概率模型。
### 4.指标分析
本次实验使用了3个评估指标，使用了ppt中提及的NMI、Homogeneity、Completeness。NMI标准互信息是探究联合熵和个体熵的关系，在一定程度上度量两个聚类结果的相似程度，homogeneity则是表示聚类后每个簇中包含的数据是同一类别的程度，completeness表示一个类中所有数据分为同一个簇的程度，通过上述三个指标对各种聚类效果进行分析。  
### 5.实验体会
通过对数据集的处理以及sklearn库中方法的调用，我更好地理解了这门data mining的实践意义，这次的数据使用的是sklearn自带的，所以不需要对数据进行清洗大大简化了处理过程，对数据的特征提取有了比较好的了解，以及如何将文本数据转化为向量。同时对各种聚类方法的原理有了更好的了解，学会如何调用sklean库，在以后遇到相似问题时可以更好地解决。



